# Transformers Inference Toolkit
ðŸ¤— [Transformers](https://github.com/huggingface/transformers) library provides great API for manipulating pre-trained NLP (as well as CV and Audio-related) models. However, preparing ðŸ¤— Transformers models for use in production usually requires additional effort. The purpose of `transformers-inference-toolkit` is to simplify automatic optimization and inference process of Huggingface Transformers-based models.

## Installation
Using `pip`:
```bash
pip install transformers-inference-toolkit
```

## Optimization
The original ðŸ¤— Transformers library includes `transformers.onnx` package, which can be used to convert PyTorch or TensorFlow models into [ONNX](https://onnx.ai/) format. This Toolkit extends this functionality by giving the user an opportunity to automatically [optimize ONNX model graph](https://onnxruntime.ai/docs/performance/graph-optimizations.html) - this is similar to what ðŸ¤— [Optimum](https://github.com/huggingface/optimum) library does, but ðŸ¤— Optimum currently has limited support for locally stored pre-trained models as well as for models of less popular architectures (for example, [MPNet](https://github.com/microsoft/MPNet)).

### Prerequisite
The Toolkit expects your pretrained model to be organized in the following way:
```bash
gpt2
â”œâ”€â”€ model
â”‚Â Â  â”œâ”€â”€ config.json
â”‚Â Â  â””â”€â”€ pytorch_model.bin
â””â”€â”€ tokenizer
    â”œâ”€â”€ merges.txt
    â”œâ”€â”€ special_tokens_map.json
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ vocab.json
```

# Work-in-progress
